# -*- coding: utf-8 -*-
"""Copy of Copy of Copy of Copy of Data wrangling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uo9gAGzJ8_DJ9V_c-jqLMXkriSs2tK0g

#Task 1: Problem Understanding
## 1)Specify the problem
## 2)Business Requirement
## 3)Literature survey

## 4)Social impact

#Task 2:Data Understanding
## 1) Data Collection
## 2) Loading Data

#Task 3:EDA
## 1) Data Cleaning
## 2)Data Manipulation
## 3)Visualization

#Task 4:Model Building
#Task 5:Testing the Model
#Task 6: Deployment
#Task 7: doc
"""

# Data collection and preparation

# Data collection

# Commented out IPython magic to ensure Python compatibility.

#importing the libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib
import pickle
# %matplotlib inline
import sklearn
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import GradientBoostingClassifier,RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import RandomizedSearchCV
import imblearn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

#checking for available styles
plt.style.available

#Applying styles to notebook
import matplotlib.pyplot as plt
import numpy as np

plt.style.use('fivethirtyeight')

# Reading csv data
df= pd.read_csv('/content/train_u6lujuX_CVtuZ9i.csv')
df.head()

test = pd.read_csv('/content/test_Y3wMUE5_7gLdaTN.csv')
df.head()

# Data preparation
# Data preprocessing

# Finding the shape of data

df.shape

test.shape

df['Loan_Status'].value_counts()

df['Loan_Status'].value_counts(normalize=True)

df['Loan_Status'].value_counts().plot.bar()

# Finding the null values

df.isnull().any()

df.isnull().sum()

df.info()

# Finding outliers
sns.boxplot(df['Credit_History'])

# Finding the count of outliers
#IQR = q3-q1..........,ub = q3+(1.5*IQR),lb = q1-(1.5*IQR)

#IQR = q3-q1

q1 =np.quantile(df['Credit_History'],0.25)
q3 =np.quantile(df['Credit_History'],0.75)

print('Q1 = {}'. format(q1))
print('Q3 = {}'. format(q3))

IQR = q3-q1

print('IQR value is {}'.format(IQR))

upperBound = q3+(1.5*IQR)
lowerBound = q3-(1.5*IQR)

print('The upper bound value is {} & the lower bound value is {}'.format(upperBound,lowerBound))

print('Skwed data :',len(df['Credit_History']>upperBound))

len(df[df['Credit_History']>upperBound])

# Handling outliers

from scipy import stats
plt.figure(figsize=(10,4))
plt.subplot(131)
sns.countplot(x= df['Credit_History'])
plt.subplot(132)
stats.probplot(np.log(y= df['Credit_History']), plot= Any)
plt.subplot(133)
sns.distplot(np.log(z= df['Credit_History']))

stats.probplot(np.log(df['Credit_History']),plot=plt)

# Transforming normal values to log values

df['Credit_History']= np.log(df['Credit_History'])

df.head()

# Encoding  
# Encoding with list comp
df['Credit_History'] =[0 if x=='LOW' else i if x=='NORMAL' else 2 for x in df['Credit_History']]

# Encoding with replace method

df['Gender'] = df['Gender'].replace({'Female':0, 'Male':1})

# Encoding with replace method

df['Education'] = df['Education'].replace({'Graduate':0, 'Not':1})

df.head()

# Spliting dependent & Independent variables

x = df.drop('Loan_Status',axis=1)
x.head()

y= df['Loan_Status']
y



# Checking columns
df.columns

# Rename columns
df.columns

df.head()

# Handling missing values

#checking data type
df.info()

df['Credit_History'].unique()

df.isnull().sum()

df.isnull().sum()*100 /len(df)

df['Credit_History'].mode([0])

df['Credit_History']= df['Credit_History'].fillna(df['Credit_History']).mode([0])

df['Self_Employed'].mode()[0]

df['Credit_History'].unique()

df['Self_Employed'].unique()

df.isnull().sum()*100 / len(df)



df.isnull().any()

df.isnull().sum()

#handling categorical columns

df.sample(5)

df['Dependents'].replace(to_replace ="3+",value='4')

df['Dependents'].unique()

df['Gender'].unique()

df['Education'].unique()

#Handling imbalance Data
x=df.drop(columns=['Married'])
y=df.Married
print(x.shape,y.shape)

"""
Types of Analysis
1) Univariate analysis
2) Bivariate analysis
3) Multivariate analysis
4)Descriptive analysis / statistics
"""

#Describtive Statistical

df.describe()

#Visual analysis
#Univariate analysis -Extracting info from a single column

#checking data distribution

sns.distplot(x= df['ApplicantIncome'])

sns.distplot(x= df(['Credit_History'])

sns.countplot(x= df['Credit_History'])

#Creating dummy dataframe for categorical values
df_cat=df.select_dtypes(include='object')
df_cat.head()

#visualizing counts in each variables

for i,j in enumerate(df_cat):
  plt.subplot(1,4,i+1)
  sns.distplot(df[j])

for i,j in enumerate(df_cat):
     print(j)
     print(i)

#Bivariate analysis
# plotting the count plot
plt.figure(figsize=(18,4))
plt.subplot(1,4,1)
sns.countplot(x=df['Gender'])
plt.subplot(1,4,2)
sns.countplot(x=df['Education'])
plt.show()

sns.countplot(x= df['Married'])

sns.countplot(x= df['Self_Employed'])

sns.countplot(x= df['Property_area'])

#Multivariate analysis -Extrct info from more than 2 columns
sns.swarmplot(x= df['Gender'])

sns.swarmplot(x= df['Applicant_Income'])

sns.swarmplot(x= df['Loan_Status'])

sns.swarmplot(x= df['Credit_History'])

# Finding correlation
f,ax= plt.subplots(figsize=(30,20))
sns.heatmap(df.corr(),annot=True,fmt=".2f",ax=ax,linewidth=0.5,linecolor="orange")
plt.xticks(rotation=45)
plt.yticks(rotation=45)
plt.show()

sns.counntplot(x= df['class'])

# Descriptive analysis-describtive stat
df.describe(include='all')

df.describe()

df.max()

df.mean()

df.min()

df.std()

pd.DataFrame(x).fillna(0)

np.nan_to_num(x)

# scaling the data
df.head()

# Independent variables
x =df.iloc[:,0:1]
x.head()

# Dependent variable
y = df.iloc[:,1:]
y.head()

# Scaling the data
from sklearn.preprocessing import StandardScaler
sc=StandardScaler()
x_bal= sc.fit_transform(x)

# Split training & testing
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=2)

# Module building

from sklearn.linear_model import LinearRegression

def DecisionTree(x_train, x_test, y_train, y_test):
    dt=DecisionTreeClassifier()
    dt.fit(x_train, y_train)
    y_Pred = dt.predict(x_test)
    print('***DecisionTreeClassifier***')
    print('Confusion matrix')
    print(confusion_matrix(y_test,y_Pred))
    print('Classification report')
    print(classification_report(y_test,y_Pred))

# Random forest model

def decisionTree(x_train, x_test, y_train, y_test):
  rf= DecisionTreeClassifier()
  rf.fit(x_train, y_train)
  yPred = df.predict(x_test)
  print('***DecisiontreeClassifier***')
  print('Confusion matrix')
  print(confusion_matrix(y_test,y_Pred))
  print('Classification report')
  print(classification_report(y_test,y_Pred))

# KNN model

def KNN(x_train, x_test, y_train, y_test):
  Knn = KNeighborsClassifier()
  Knn.fit(x_train, y_train)
  yPred = Knn.predict(x_test)
  print('***KNeighborsClassifier***')
  print('Confusion matrix')
  print(confusion_matrix(y_test, yPred))
  print('Classification report')
  print(classification_report(y_test, yPred))

# Xgboost moedel
def xgboost(x_train, x_test, y_train, y_test):
  xg = GradientBoostingClassifier()
  xg.fit(x_train, y_train)
  yPred = xg.predict(x_test)
  print('***GradientBoostingClassifier***')
  print('Confusion matrix')
  print(confusion_matrix(y_test, yPred))
  print('Classification report')
  print(classification_report(y_test, yPred))

#ANN model
#Importing the keras libraries and packages
import tensorflow
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Initialising the ANN
classifier=Sequential()

# Additing the input layer and the first hidden layer
classifier.add(Dense (units=100, activation= 'relu',input_dim= 11))

#Adding the second hidden layer
classifier.add(Dense(units=50, activation='relu'))

#Adding the output layer
classifier.add(Dense(units=1, activation='sigmoid'))

# Compiling the ANN
classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

#.Fitting.the.ANN.to.the.Training.set
model_history= classifier.fit(x_train, y_train, batch_size=100, validation_split= 0.2,epochs= 100)

# Gender Married Dependents Education Self_Employed ApplicantIncome CoapplicantIncome LoanAmount Loan_Amount_Term Credit_History Property_Area

dtr.predict([[1,1,0,1,1,4276,1542,145,240,0,1]])

#Gender Married Dependents Education Self_Employe ApplicantIncome CoapplicantIncome LoanAmount Loan_Amount_Term Credit_History Properly_Area
rfr.predict([[1,1,0,1,1,4276,1542,145,240,0,1]])

KNeighbors.predictions([[1,1,0,1,1,4276,1542,145,240,0,1]])

xgb.predict([[1,1,0,1,1,4276,1542,145,240,0,1]])

classifier.save("loan.h5")

y_pred = classifier.predict(x_test)
y_pred
y_pred = (y_pred> 0.5)
y_pred

def predict_exit(sample_value):
  sample_value=np.array(sample_value)
  sample_value=sample_value.reshape(1,-1)
  sample_value=sc.transform(sample_value)
  return classifier.predict(sample_value)

sample_value= [[1,1,0,1,1,4276,1542,145,240,0,1]]
if predict_exit(sample_value)>0.5:
  print('prediction: High chance of Loan Approval!')
else:
  print('prediction: Low chance of Loan Approval.')

# performence Testing & Hyperparameter Tunning

def CompareModel(x_tarin,x_test,y_train,y_test):
  decisionTree(x_train,x_test,y_train,y_test)
  print('-'*100)
  RandomForest(x_train,x_test,y_train,y_test)
  print('-'*100)
  XGB(x_train,x_test,y_train,y_test)
  print('-'*100)
  KNN(x_train,x_test,y_train,y_test)
  print('-'*100)

y_Pred = classifier.predict(x_test)
print(accuracy_score(y_pred,y_test))
print("ANN Model")
print("Confusion_Matrix")
print(confusion_Matrix(y_test,y_pred))
print("Classification Report")
print(classification_report(y_test,y_pred))

